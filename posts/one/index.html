<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
  Critical Analysis on Batch Normalization paper · Kazi Hasan
</title>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">


<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net/; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">




<meta name="author" content="Kazi Hasan Ibn Arif">
<meta name="description" content="Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Link to heading Paper link: https://arxiv.org/abs/1502.03167
Problem Statement Link to heading Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard.">
<meta name="keywords" content="blog,developer,personal,machine,Learning,Engineer,phd,student">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Critical Analysis on Batch Normalization paper"/>
<meta name="twitter:description" content="Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Link to heading Paper link: https://arxiv.org/abs/1502.03167
Problem Statement Link to heading Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard."/>

<meta property="og:title" content="Critical Analysis on Batch Normalization paper" />
<meta property="og:description" content="Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Link to heading Paper link: https://arxiv.org/abs/1502.03167
Problem Statement Link to heading Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://hasanibnarif.github.io/posts/one/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-25T21:55:45+06:00" />
<meta property="article:modified_time" content="2022-06-25T21:55:45+06:00" />





<link rel="canonical" href="http://hasanibnarif.github.io/posts/one/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.c4d7e93a158eda5a65b3df343745d2092a0a1e2170feeec909b8a89443903c6a.css" integrity="sha256-xNfpOhWO2lpls980N0XSCSoKHiFw/u7JCbiolEOQPGo=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css" integrity="sha256-OeQafxa9&#43;MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">




<meta name="generator" content="Hugo 0.100.1" />





  </head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Kazi Hasan
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/cv/">CV</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://hasanibnarif.github.io/posts/one/">
              Critical Analysis on Batch Normalization paper
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2022-06-25T21:55:45&#43;06:00">
                June 25, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              5-minute read
            </span>
          </div>
          <div class="authors">
  <i class="fa fa-user" aria-hidden="true"></i>
    <a href="/authors/kazi-hasan/">Kazi Hasan</a></div>

          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/critial-analysis/">Critial Analysis</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/deep-learning/">Deep Learning</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/batch-normalization/">Batch Normalization</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h3 id="todays-paper--batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">
  Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
  <a class="heading-link" href="#todays-paper--batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Paper link: <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p>
<h3 id="problem-statement">
  <strong>Problem Statement</strong>
  <a class="heading-link" href="#problem-statement">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard. In a deep neural network, there are several layers. Each layer has nodes with learnable parameters and get the input from the previous layer. But after going through the non linear computation of each layer, the input distribution get completely changed. This situation is called internal covariate shift. This paper address internal covariate shift by normalizing layer input in each layer. The normalization process previously applied only to the input layer but this paper suggests to make normalization as a part of the model architecture. Although this is a simple solution, it improves deep learning model like ImageNet with a huge margine.</p>
<h3 id="current-approach">
  <strong>Current Approach</strong>
  <a class="heading-link" href="#current-approach">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>If we consider a neural network like this</p>
<p>$ℓ = F2(F1(u, Θ1), Θ2)$</p>
<p>Here, Θ1 is parameters from the first layer and the loss the first layer is F1(u, Θ1) which is the input of the following layer with parameter Θ2 and the final loss is F2(F1(u, Θ1), Θ2). For a mini batch size m, the gradiant descent step is :</p>
<p>$Θ2 ← Θ2 − \frac{α}{m} \sum_{i=1}^{m} \frac{∂F2(x_i,Θ_2)}{∂Θ_2}$</p>
<p>This method is called gradiant descent which is an iterative method to compute gradiant in each layer of a deep neural network.</p>
<p>The layers outside the subnetwork would benefit from a similar distribution of inputs to the subnetwork. SGD would be less likely to become stuck in the saturated regime and the training would continue more speedily if we could make the distribution of nonlinearity inputs more stable while the network trains. So elimination of internal covariate shift is the key here.</p>
<p>Firstly, This paper propose a new mechanism called batch normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It reduces the dependencies of gradiant scale because it aggregates a batch of inputs from the trainset. It also allow us to use much bigger learning rate. Also, it makes it possible to use saturating nonlinearities by preventing the network from getting stuck.</p>
<p>The problem with gradient descent optimization is that it ignores the fact that normalization occurs. In order to solve this problem, the network must always produce activations with the appropriate distribution, regardless of the parameter values. By doing this, the normalization and its dependency on the model parameters could both be accounted for by the gradient of the loss with respect to the model parameters.</p>
<p>Normalization of a mini batch input on each layer is expensive. This paper suggest to normalize scaler feature to zero mean and variance of 1. If x is the input of a mini-batch, then normalization is</p>
<p>$\hat{x}^{k} = \frac{{x}^{k}-E[{x}^{k}]}{\sqrt{Var[{x}^{k}]}}$</p>
<p>But simply normalize of a layer may change the input completely so the model is given a room for improvement. To battle this, this paper introduce two new learnable parameters γ and β.</p>
<p>These parameters are learned along with the original parameters which gives the model to restore the representation of input layer like the model can bring back to its original activations if that were the optimal thing to do.</p>
<p>Lets consider a mini-batch B of size m and BN is the Batch Normalizing Transform. So the <strong>algorithm</strong> of BN is here -</p>
<p><img src="/Critical%20Analysis%20of%20Batch%20Normalization%20977f3b39b94b418c9433634346ec19cc/Untitled.png" alt="Untitled img"></p>
<h3 id="experimentation-setup">
  <strong>Experimentation Setup</strong>
  <a class="heading-link" href="#experimentation-setup">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Batch Normalization can be applied to any set of activation in a network. This paper looked at the issue of predicting the digit class on the <strong>MNIST</strong> dataset to confirm the impacts of internal covariate shift on training and the effectiveness of batch normalization to mitigate it. It used a simple network with 28X28 binary image as input and 3 fully connected hidden layer with 100 activation each. The goal is the compare between the baseline and BN network of MNIST dataset.</p>
<h3 id="results">
  <strong>Results</strong>
  <a class="heading-link" href="#results">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The result of the experiment shows that, the validation accuracy of the network converges much faster and smoothly when BN is used compared to the baseline model. By only using Batch Normalization (BN-Baseline), this paper match the accuracy of Inception in less than half the number of training steps. After applying few modifications,it significantly increase the training speed of the network. BN-x5 needs 14 times fewer steps than Inception to reach the 72.2% accuracy. Interestingly, increasing the learning rate further (BN-x30) causes the model to train somewhat slower initially, but allows it to reach a higher final accuracy. It reaches 74.8% after 6·10^6 steps, i.e. 5 times fewer steps than required by Inception to reach 72.2%</p>
<p><img src="/Critical%20Analysis%20of%20Batch%20Normalization%20977f3b39b94b418c9433634346ec19cc/Untitled%201.png" alt="Untitled"></p>
<h3 id="conclusion">
  <strong>Conclusion</strong>
  <a class="heading-link" href="#conclusion">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Although this paper is a bit older, but this is still relevent. Batch normalization is widely practised in various machine learning approach today. It providesa novel mechanism which dramatically improve the training of deep learning networks. This paper incorporate normalization process in the architecture and made it learable by adding only two extra parameters. This also enables few more possibilities like if we use BN, we do not need dropout layer because BN itself works as regularization technique.</p>
<p>Batch normalization solves a major problem called <strong>internal covariate shift.</strong> It helps by making the data flowing between intermediate layers of the neural network look, this means we can use a higher learning rate. It has a regularizing effect which means you can often remove dropout.</p>
<p>Although batch normalization speeds-up training and generalization significantly in CNN, there is not indication that this approach will also work well with other deep learning models like RNN. This paper only shows its effectiveness for CNN architecture with a shallow network. So, the effectiveness measure is not comprehensive.</p>
<p>Also, there are few other normalization techniques where batch normalization techniques could be tested, for example</p>
<ul>
<li>Layer Normalization.</li>
<li>Instance Normalization.</li>
<li>Group Normalization (+ weight standardization).</li>
</ul>
<p>Apart from this, batch normalization is a milestone technique in the development of deep learning. However, normalizing along the batch dimension introduces some problems as discussed, which suggests that there’s still room for improvement in normalization techniques.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2019 -
    
    2022
     Kazi Hasan Ibn Arif 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.f411a1043e37c7c14dfb03f4d94d60d9ee69cfa413b16d0fd4f28695babb82bb.js" integrity="sha256-9BGhBD43x8FN&#43;wP02U1g2e5pz6QTsW0P1PKGlbq7grs="></script>
  

  

  

  

  

  

  

  

  

  
</body>

</html>
