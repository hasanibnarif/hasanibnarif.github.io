<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Kazi Hasan</title>
    <link>http://hasanibnarif.github.io/posts/</link>
    <description>Recent content in Posts on Kazi Hasan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 25 Sep 2022 17:25:50 +0600</lastBuildDate><atom:link href="http://hasanibnarif.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</title>
      <link>http://hasanibnarif.github.io/posts/three/</link>
      <pubDate>Sun, 25 Sep 2022 17:25:50 +0600</pubDate>
      
      <guid>http://hasanibnarif.github.io/posts/three/</guid>
      <description>Paper Link: https://arxiv.org/pdf/1509.02971.pdf
Problem Statement Link to heading DQN was able to solve problems in high-dimensional observation spaces, but action spaces neeeded to be discrete and low-dimensional because each step optimizes over state values. This makes DQN not useful for robotic walking for example. The paper relies on Deep Reinforcement Learning to find multiple coupling action and observation space mappings simultaneously. Physical control task as mentioned have continuous, high-dimensional action spaces.</description>
    </item>
    
    <item>
      <title>Critical Analysis on Deep Residual Learning for Image Recognition</title>
      <link>http://hasanibnarif.github.io/posts/two/</link>
      <pubDate>Mon, 15 Aug 2022 22:01:23 +0600</pubDate>
      
      <guid>http://hasanibnarif.github.io/posts/two/</guid>
      <description>Paper Link: here
Problem Statement Link to heading Introduction
The more deeper a deep neural network is, the more computational complexity is added to the training process. For example, a training a deep neural network may face exploding or vanishing gradiant problem, which result in compromising accuracy of the model. This paper introduced a new technique to combat the problem of training a deep neural network. They present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.</description>
    </item>
    
    <item>
      <title>Critical Analysis on Batch Normalization paper</title>
      <link>http://hasanibnarif.github.io/posts/one/</link>
      <pubDate>Sat, 25 Jun 2022 21:55:45 +0600</pubDate>
      
      <guid>http://hasanibnarif.github.io/posts/one/</guid>
      <description>Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Link to heading Paper link:Â https://arxiv.org/abs/1502.03167
Problem Statement Link to heading Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard.</description>
    </item>
    
  </channel>
</rss>
