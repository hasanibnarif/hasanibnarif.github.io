<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kazi Hasan</title>
    <link>http://hasanibnarif.github.io/authors/kazi-hasan/</link>
    <description>Recent content on Kazi Hasan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 Jun 2022 21:55:45 +0600</lastBuildDate><atom:link href="http://hasanibnarif.github.io/authors/kazi-hasan/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Critical Analysis on Batch Normalization paper</title>
      <link>http://hasanibnarif.github.io/posts/one/</link>
      <pubDate>Sat, 25 Jun 2022 21:55:45 +0600</pubDate>
      
      <guid>http://hasanibnarif.github.io/posts/one/</guid>
      <description>Todays Paper : Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Link to heading Paper link:Â https://arxiv.org/abs/1502.03167
Problem Statement Link to heading Stochastic gradient descent (SGD) is one of the effective ways of training deep networks, and SGD variants such as momentum etc. Batch normalization improves SGD and comes with few advanteages. At the same time, this approach comes with some issues like it requires more sophisticated parameter initilalization and make the training process hard.</description>
    </item>
    
  </channel>
</rss>
